# 数据挖掘导论 5.4 人工神经网络
+ 激活函数、学习率（可自适应、随着迭代的增加而减小）
+ 多层神经网络，隐藏层，隐藏节点——处理一些非线性的问题
+ 前馈式、递归式
+ 最小化误差的平方和
+ 反向传播
+ 特点：
  + 普适近似，即可以用来近似任何目标函数
  + 对噪声敏感（但是在《机器学习》里提到，可以容忍噪声的，这两个观点矛盾了。暂时偏向后者）
  + 收敛到局部最小值——动量项
+ 这一节讲得太概念化，补充复习下《机器学习》的：
  + 感知器训练法则：从随机值开始，反复地应用到训练样例，只要它错误分类阳历就修改感知器的权值。
  + 梯度下降法则：以最小化误差为目标，通过求偏导的方法，明确梯度下降时更新权值的函数。
    + 标准梯度下降：积累整个训练样例的权值变化，一次性更新
    + 增量梯度下降：根据每个单独样例的误差增量计算权值更新（这里卡住了，因为表达式与上面的感知器训练法则一样的！再思考）
  + 对线性可分的样例，感知器法则可以。但对非线性可分的，感知器法则不收敛，而梯度下降法则没这限制

# TODO
1. 增量梯度下降，与感知器训练法则的区别，除了停止的方式不同外，在表达式上有什么不同，还得再思考。
1. 明天继续反响传播的章节
